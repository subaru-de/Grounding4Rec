{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./info/Books_5_2017-10-2018-11.txt', 'r')\n",
    "books = f.readlines()\n",
    "item_names = [_.split('\\t')[0] for _ in books]\n",
    "item_ids = [_.split('\\t')[1][:-1] for _ in books]\n",
    "item_dict = dict(zip(item_ids, item_names))\n",
    "# id_mapping = dict(zip(item_ids, range(len(item_ids))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = item_names[0]\n",
    "\n",
    "import sys\n",
    "\n",
    "import fire\n",
    "import gradio as gr\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "import transformers\n",
    "import json\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "try:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "except:  # noqa: E722\n",
    "    pass\n",
    "\n",
    "def main(\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"Qwen/Qwen1.5-0.5B\",\n",
    "    lora_weights: str = \"../../Qwen1.5-0.5B/lora-alpaca\",\n",
    "    test_data_path: str = \"./test_128.json\",\n",
    "    result_json_data: str = \"./result.json\",\n",
    "    batch_size: int=8,\n",
    "):\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='decapoda-research/llama-7b-hf'\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "    if device == \"cuda\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            load_in_8bit=load_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map={'': 0}\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            device_map={\"\": device},\n",
    "        )\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    name_ids = [tokenizer.encode(name, add_special_tokens=False)]\n",
    "    print(name_ids)\n",
    "\n",
    "    prompt = [generate_prompt(\"Given a list of books the user has read before, please recommend a new book that the user likes to read.\", \"The user has read the following books before:\\\"Understanding Elizabeth\\\", \\\"Earthly Remains: A Commissario Guido Brunetti Mystery\\\", \\\"The Case of the Green-Dressed Ghost (Dr Ribero's Agency of the Supernatural)\\\", \\\"Digging In: A Novel\\\", \\\"Alone with Mr. Darcy: A Pride & Prejudice Variation\\\", \\\"The Crusader's Bride: The Champions of Saint Euphemia Book 1\\\", \\\"My Lady Thief\\\", \\\"A Quiet Life in the Country (A Lady Hardcastle Mystery)\\\", \\\"The Unwanted Heiress (The Archer Family Regency Romances) (Volume 1)\\\", \\\"The Irish Inheritance: A Jayne Sinclair Genealogical Mystery\\\"\\n \")]\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, add_special_tokens=False).to(device)\n",
    "    print(inputs.input_ids)\n",
    "\n",
    "\n",
    "def generate_prompt(instruction, input=None):\n",
    "    if input:\n",
    "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  \n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.  \n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "def csv_to_json(input_path, output_path, sample=False):\n",
    "    data = pd.read_csv(input_path)\n",
    "    if sample:\n",
    "        data = data.sample(n=40000, random_state=42).reset_index(drop=True)\n",
    "        data.to_csv(output_path[:-5] + \".csv\", index=False)\n",
    "    json_list = []\n",
    "    data = data[:40000]\n",
    "    data.to_csv(output_path[:-5] + \".csv\", index=False)\n",
    "    for index, row in data.iterrows():\n",
    "        row['history_item_id'] = eval(row['history_item_id'])\n",
    "        row['history_item_title'] = eval(row['history_item_title'])\n",
    "        L = len(row['history_item_id'])\n",
    "        history = \"The user has read the following books before:\"\n",
    "        for i in range(L):\n",
    "            if i == 0:\n",
    "                history += \"\\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "            else:\n",
    "                history += \", \\\"\" + row['history_item_title'][i] + \"\\\"\"\n",
    "        target_item_name = \"\\\"\" + item_dict[str(row['item_id'])] + \"\\\"\"\n",
    "        json_list.append({\n",
    "            \"instruction\": \"Given a list of books the user has read before, please recommend a new book that the user likes to read.\",\n",
    "            \"input\": f\"{history}\\n \",\n",
    "            \"output\": target_item_name,\n",
    "        })\n",
    "        \n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(json_list, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_to_json('./train/Books_5_2017-10-2018-11.csv', './train.json')\n",
    "# csv_to_json('./valid/Books_5_2017-10-2018-11.csv', './valid.json')\n",
    "# csv_to_json('./test/Books_5_2017-10-2018-11.csv', './test.json')\n",
    "# csv_to_json('./valid/Books_5_2017-10-2018-11.csv', './valid_5000.json', sample=True)\n",
    "# csv_to_json('./test/Books_5_2017-10-2018-11.csv', './test_5000.json', sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_to_json('./train/Books_5_2017-10-2018-11.csv', './train_1024.json', sample=True)\n",
    "# csv_to_json('./valid/Books_5_2017-10-2018-11.csv', './valid_128.json', sample=True)\n",
    "# csv_to_json('./test/Books_5_2017-10-2018-11.csv', './test_128.json', sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_to_json('./train/Books_5_2017-10-2018-11.csv', './train_40000.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59fa845e12d05d721e6f4368480cbf49d04f4a649a02e83c3e47bffdee3cc61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
